{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION\n",
    "\n",
    "### Aim:\n",
    "\n",
    "To construct a method to classify the unlabelled sentences according to the labels given in few articles (training set).\n",
    "\n",
    "###  Introduction:\n",
    "\n",
    "This corpus contains sentences from the abstract and introduction of 30 scientific articles that have been annotated (i.e. labeled or tagged) according to a modified version of the Argumentative Zones annotation scheme. These 30 scientific articles come from three different domains:\n",
    "\n",
    "1. PLoS Computational Biology (PLOS)\n",
    "2. The machine learning repository on arXiv (ARXIV)\n",
    "3. The psychology journal Judgment and Decision Making (JDM)\n",
    "\n",
    "There are 10 articles from each domain. In addition to the labeled data, this corpus also contains a corresponding set of unlabeled articles. These unlabeled articles also come from PLOS, ARXIV, and JDM. There are 300 unlabeled articles from each domain (again, only the sentences from the abstract and introduction). These unlabeled articles can be used for unsupervised or semi-supervised approaches to sentence classification which rely on a small set of labeled data and a larger set of unlabeled data.\n",
    "\n",
    "Labels in the article includes AIMX, OWNX, CONT, BASE, MISC\n",
    "\n",
    "### Methods:\n",
    "\n",
    "#### Text Pre-processing\n",
    "1. Extracted the labelled text files (training set) from the folder and joined all the sentences to form a dataframe.\n",
    "2. All the sentences and its corresponding categories are separated.\n",
    "3. Stopwords are removed and lematized to increase classsification accuracy.\n",
    "4. Counter vectorized the data for classification.\n",
    "4. Words in the word_list folder are extracted and labelled with the corresponding categories.\n",
    "5. Above words are lematizzed and added to the counter vectorized output of data.\n",
    "\n",
    "##### Naive Bayes:\n",
    "6. Unlabelled data (test data) is extracted and preprocessed as mentioned above.\n",
    "7. Naive bayes classifier is used for classification since we have small training set and large testing set, this gives better accuracy.\n",
    "8. Accuracy of 87% is achieved with Navie Bayes classification.\n",
    "9. Further to improve accuracy of text classification XG Boost is used.\n",
    "\n",
    "##### XG Boost:\n",
    "10. Using XG Boost method the accuracy is increased to 92.87%\n",
    "\n",
    "#### Future Work:\n",
    "At the end I used grid search to improve the XG boost parameters. But the running time was too long and not efficient.\n",
    "We may improve the classification by using some deep learning algorithms also in future.\n",
    "\n",
    "##### Prerequisite  libraries to instal in python before running\n",
    "\n",
    "nltk, xgboost\n",
    "\n",
    "### Conslusion:\n",
    "\n",
    "Thus, the classifier is designed with an accuracy of 92.87% and the resulted output with labelled text of 300 unlabelled articles. The resulted output is saved in a text file with filename \"belled_prediction.txt\".\n",
    "\n",
    "### Python Code\n",
    "Before running the code, change the path of input files where ever required.\n",
    "\n",
    "Example:\n",
    "My path includes: \"C:/Users/naren/Documents/Divya/SentenceCorpus/labeled_articles\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing wordlist\n",
    "aim_word_list=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/aim.txt\",header=None)\n",
    "aim_words=list(aim_word_list[0])\n",
    "\n",
    "own_word_list=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/own.txt\",header=None)\n",
    "own_words=list(own_word_list[0])\n",
    "\n",
    "base_word_list=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/base.txt\",header=None)\n",
    "base_words=list(base_word_list[0])\n",
    "\n",
    "cont_word_list=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/contrast.txt\",header=None)\n",
    "cont_words=list(cont_word_list[0])\n",
    "\n",
    "vocab=aim_words+own_words+base_words+cont_words\n",
    "vocab=list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all txt files\n",
    "path=r\"C:/Users/naren/Documents/Divya/SentenceCorpus/labeled_articles\"\n",
    "labeled_articles=[pd.read_table(file,error_bad_lines=False,warn_bad_lines=False) for file in glob.glob(path+\"/*.txt\")]\n",
    "\n",
    "#concat all the files into one daatframe\n",
    "df_labeled=pd.concat(labeled_articles)\n",
    "df_labeled=df_labeled.reset_index()\n",
    "\n",
    "#renaming the columns\n",
    "df_labeled.columns=[\"category\",\"text\"]\n",
    "\n",
    "#changing the order of the columns\n",
    "df_labeled=df_labeled[[\"text\",\"category\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text        43\n",
      "category    0 \n",
      "dtype: int64\n",
      "['### introduction ###']\n"
     ]
    }
   ],
   "source": [
    "#checking for missing values\n",
    "print(df_labeled.isnull().sum())\n",
    "\n",
    "#As we see the category is introduction, we better delete the null value\n",
    "print(df_labeled[df_labeled[\"text\"].isnull()][\"category\"].unique())\n",
    "\n",
    "#removing null values\n",
    "df_labeled=df_labeled.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MISC' 'AIMX' 'OWNX' 'CONT' 0L 1L 2L 3L 4L 5L 6L 7L 8L 9L 10L 11L 12L 13L\n",
      " 14L 15L 16L 17L 18L 19L 20L 21L 22L 23L 24L 25L 26L 27L 28L 29L 30L 31L\n",
      " 32L 33L 34L 35L 36L 37L 38L 39L 40L 41L 42L 43L 44L 45L 46L 47L 48L 49L\n",
      " 50L 51L 52L 53L 54L 55L 56L 57L 58L 59L 60L 61L 62L 63L 64L 65L 66L 67L\n",
      " 'BASE' 'OWNX ' 68L 69L 70L 71L 72L 73L 74L 75L 76L 77L]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>OWNX The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>MISC Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>AIMX In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>AIMX In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>MISC Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                  text  \\\n",
       "136  OWNX The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied                                       \n",
       "273  MISC Although the Internet AS-level topology has been extensively studied over the past few years, little is known about the details of the AS taxonomy             \n",
       "333  AIMX In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model                                            \n",
       "358  AIMX In this paper we derive the equations for Loop Corrected Belief Propagation on a continuous variable Gaussian model                                            \n",
       "411  MISC Defensive forecasting is a method of transforming laws of probability (stated in game-theoretic terms as strategies for Sceptic) into forecasting algorithms   \n",
       "\n",
       "    category  \n",
       "136  0        \n",
       "273  0        \n",
       "333  0        \n",
       "358  0        \n",
       "411  0        "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for unique categories in category column\n",
    "print(df_labeled[\"category\"].unique())\n",
    "\n",
    "#if you look at the unique categories,\n",
    "#there are many categories which are not specified by annotators,lets inspect those rows\n",
    "df_labeled[df_labeled[\"category\"]==0].head()\n",
    "#looking at those rows we conclude that text data has to be processed to be categorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the data \n",
    "cat_labels=['AIMX','OWNX','MISC','CONT','BASE','OWNX']\n",
    "df_label=df_labeled[df_labeled[\"category\"].isin(cat_labels)]\n",
    "df_to_be_labeled=df_labeled[~df_labeled[\"category\"].isin(cat_labels)]\n",
    "\n",
    "#data processing for df_to_be_labled\n",
    "del df_to_be_labeled[\"category\"]\n",
    "df_to_be_labeled[\"category\"]=[x[:4] for x in df_to_be_labeled[\"text\"] ]\n",
    "df_to_be_labeled[\"text\"]=[x[4:] for x in df_to_be_labeled[\"text\"] ]\n",
    "\n",
    "#concatenating data\n",
    "labeled_df=pd.concat([df_label,df_to_be_labeled])\n",
    "\n",
    "labeled_df=labeled_df[labeled_df[\"category\"]!='### ']\n",
    "\n",
    "#replacing the values of OWNX\n",
    "labeled_df[\"category\"].replace('OWNX ','OWNX',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a si': 5, 'CONT': 2, 'AIMX': 0, 'MISC': 3, 'the ': 6, 'BASE': 1, 'OWNX': 4}\n"
     ]
    }
   ],
   "source": [
    "#create another dataframe\n",
    "df=labeled_df.copy()\n",
    "\n",
    "# remove punctuation from text\n",
    "df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#change text to lower case\n",
    "df[\"text\"]=df[\"text\"].map(lambda x: x.lower())\n",
    "\n",
    "#remove stopwords\n",
    "stop_words_txt=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/stopwords.txt\",header=None)\n",
    "stop_words=list(stop_words_txt[0])\n",
    "df['text'] = df[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "#tokenization\n",
    "df['word_tokenize'] = df[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "#lemmatization\n",
    "lmtzr = WordNetLemmatizer()\n",
    "df['lemmatized'] = df['word_tokenize'].apply(lambda x: [lmtzr.lemmatize(y) for y in x])\n",
    "\n",
    "#lemmatization of vocab\n",
    "vocab_lemmatized=[lmtzr.lemmatize(x) for x in vocab]\n",
    "#stemming\n",
    "#ps=PorterStemmer()\n",
    "#df['stemmed'] = df['word_tokenize'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "df[\"new_text\"]=df['lemmatized'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Label Encoding the category variable\n",
    "le=LabelEncoder()\n",
    "df.category=le.fit_transform(df.category)\n",
    "le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(le_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading test files\n",
    "path_test=r\"C:/Users/naren/Documents/Divya/SentenceCorpus/unlabeled_articles/\"\n",
    "\n",
    "arxiv_articles=[pd.read_table(file,error_bad_lines=False,warn_bad_lines=False,engine=\"python\") for file in glob.glob(path_test+\"arxiv_unlabeled/*.txt\")]\n",
    "jdm_articles=[pd.read_table(file,error_bad_lines=False,warn_bad_lines=False,engine=\"python\") for file in glob.glob(path_test+\"jdm_unlabeled/*.txt\")]\n",
    "plos_articles=[pd.read_table(file,error_bad_lines=False,warn_bad_lines=False,engine=\"python\") for file in glob.glob(path_test+\"plos_unlabeled/*.txt\")]\n",
    "\n",
    "arxiv=pd.concat(arxiv_articles)\n",
    "jdm=pd.concat(jdm_articles)\n",
    "plos=pd.concat(plos_articles)\n",
    "df_unlabelled=pd.concat([arxiv,jdm,plos])\n",
    "\n",
    "#changing the name of the column\n",
    "df_unlabelled.columns=[\"text\"]\n",
    "\n",
    "#finding removing null values\n",
    "#print(df_unlabelled.isnull().sum())\n",
    "df_unlabelled=df_unlabelled.dropna()\n",
    "\n",
    "#removing duplicates\n",
    "df_unlabelled=df_unlabelled.drop_duplicates(keep=\"first\")\n",
    "df_unlabelled=df_unlabelled[df_unlabelled[\"text\"] !=\"### introduction ###\"]\n",
    "\n",
    "#create another dataframe\n",
    "df_u=df_unlabelled.copy()\n",
    "\n",
    "# remove punctuation from text\n",
    "df_u[\"text\"] = df_u['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "#change text to lower case\n",
    "df_u[\"text\"]=df_u[\"text\"].map(lambda x: x.lower())\n",
    "\n",
    "#remove stopwords\n",
    "stop_words_txt=pd.read_csv(\"C:/Users/naren/Documents/Divya/SentenceCorpus/word_lists/stopwords.txt\",header=None)\n",
    "stop_words=list(stop_words_txt[0])\n",
    "df_u['text'] = df_u[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "#tokenization\n",
    "df_u['word_tokenize'] = df_u[\"text\"].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "#lemmatization\n",
    "lmtzr = WordNetLemmatizer()\n",
    "df_u['lemmatized'] = df_u['word_tokenize'].apply(lambda x: [lmtzr.lemmatize(y) for y in x])\n",
    "\n",
    "#stemming\n",
    "#ps=PorterStemmer()\n",
    "#df['stemmed'] = df['word_tokenize'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "df_u[\"new_text\"]=df_u['lemmatized'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the term frequency\n",
    "count_vect = CountVectorizer(min_df=0.001,ngram_range=(1,2))\n",
    "df_counts = count_vect.fit_transform(df.new_text)\n",
    "count=count_vect.get_feature_names()\n",
    "\n",
    "#combining the most frequency words and the word list given \n",
    "new_vocab=set(count+vocab_lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3116, 3650)\n"
     ]
    }
   ],
   "source": [
    "#tfidf vectorization\n",
    "vectorizer = TfidfVectorizer(vocabulary=set(count))\n",
    "transformed = vectorizer.fit_transform(df.new_text)\n",
    "print(transformed.shape)\n",
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### 1.Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89569961489088579"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0.01).fit(transformed, df.category)\n",
    "\n",
    "#predicting on train data\n",
    "predicted = clf.predict(transformed)\n",
    "np.mean(predicted == df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MISC    29354\n",
       "OWNX    4217 \n",
       "CONT    291  \n",
       "AIMX    5    \n",
       "Name: label_category, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the train data vocab on test data\n",
    "df_test_tfidf = vectorizer.transform(df_u.new_text)\n",
    "\n",
    "#predicting on test data\n",
    "y_preds=clf.predict_proba(df_test_tfidf )\n",
    "y_preds=pd.DataFrame(y_preds)\n",
    "y_preds[\"max\"]=y_preds.apply(lambda x: x.argmax(), axis=1)\n",
    "df_u[\"category\"]=y_preds[\"max\"]\n",
    "df_u[\"category\"]=df_u[\"category\"].astype(\"object\")\n",
    "df_u[\"label_category\"]=df_u[\"category\"].replace({0:'AIMX',1:'BASE', 2:'CONT',3:'MISC', 4:'OWNX'})\n",
    "\n",
    "#test data labelled\n",
    "\n",
    "test_unlabelled=df_u[[\"label_category\"]]\n",
    "test_unlabelled[\"text\"]=df_unlabelled[\"text\"]\n",
    "\n",
    "test_unlabelled[\"label_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MISC    1824\n",
       "OWNX    865 \n",
       "AIMX    194 \n",
       "CONT    170 \n",
       "BASE    61  \n",
       "a si    1   \n",
       "the     1   \n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'multi:softprob',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "xg_boost=xgb1.fit( transformed, df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92875481386392811"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#predicting on train data\n",
    "predicted_xgb = xg_boost.predict(transformed)\n",
    "np.mean(predicted_xgb == df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MISC    21224\n",
       "OWNX    6257 \n",
       "AIMX    4820 \n",
       "CONT    1566 \n",
       "Name: label_category, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting on test data\n",
    "y_preds=xg_boost.predict_proba(df_test_tfidf )\n",
    "y_preds=pd.DataFrame(y_preds)\n",
    "y_preds[\"max\"]=y_preds.apply(lambda x: x.argmax(), axis=1)\n",
    "df_u[\"category\"]=y_preds[\"max\"]\n",
    "df_u[\"category\"]=df_u[\"category\"].astype(\"object\")\n",
    "df_u[\"label_category\"]=df_u[\"category\"].replace({0:'AIMX',1:'BASE', 2:'CONT',3:'MISC', 4:'OWNX'})\n",
    "\n",
    "#test data labelled\n",
    "\n",
    "test_unlabelled=df_u[[\"label_category\"]]\n",
    "test_unlabelled[\"text\"]=df_unlabelled[\"text\"]\n",
    "test_unlabelled[\"label_category\"].value_counts()\n",
    "test_unlabelled.to_csv('C:/Users/naren/Documents/Divya/SentenceCorpus/Labelled_Prediction.txt',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \"#from sklearn.grid_search import GridSearchCV \\n\",\n",
    "    \"#param_test1 = {\\n\",\n",
    "    \"# 'max_depth':[3,4,5,6,7,8,9,10],\\n\",\n",
    "    \"## 'min_child_weight':[1,2,3,4,5,6,7,8,9,10,11,12],\\n\",\n",
    "    \"#'gamma':[i/10.0 for i in range(0,5)]\\n\",\n",
    "    \"#}\\n\",\n",
    "    \"#gsearch = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, \\n\",\n",
    "    \"                                                     max_depth=5,\\n\",\n",
    "    \"# min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\\n\",\n",
    "    \"# objective= 'multi:softmax', nthread=4, scale_pos_weight=1, seed=27), \\n\",\n",
    "    \"# param_grid = param_test1, scoring='accuracy',n_jobs=4,iid=False, cv=5)\\n\",\n",
    "    \"#grid_search=gsearch.fit(transformed, df.category)\\n\",\n",
    "    \"#gsearch.grid_scores_, gsearch.best_params_, gsearch.best_score_\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
